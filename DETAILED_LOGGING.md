# 详细日志说明文档

## 📊 详细日志功能概述

本项目新增了详细的注意力机制计算日志功能，可以一步步查看Transformer的学习过程。

## 🎯 记录的详细信息

### 1. **词嵌入 (Embedding) 过程**
```
- 输入token IDs
- Embedding后的形状和统计
- 缩放因子 sqrt(d_model)
- 位置编码后的统计
```

### 2. **注意力机制详细计算**

#### Q、K、V矩阵
```
- QKV变换后的形状
- Q矩阵统计: 范围、均值、标准差
- K矩阵统计: 范围、均值、标准差
- V矩阵统计: 范围、均值、标准差
- Head 0 的 Q 矩阵前3个token的前5维数值
```

#### 注意力分数计算
```
- 注意力分数 (Q @ K^T) 的完整9×9矩阵
- mask前的分数矩阵
- mask后的分数矩阵 (未来位置显示为-inf)
- 缩放因子 sqrt(d_k)
```

#### 注意力权重 (Softmax)
```
- Softmax后的完整注意力权重矩阵
- 每行和验证 (应该都等于1.0)
- 展示因果mask的效果（下三角矩阵）
```

### 3. **损失计算**
```
- 损失值 (精确到6位小数)
- 输出logits的数值
- Top5预测概率和对应词汇
```

### 4. **梯度统计**
```
对关键层记录:
- embedding.weight 的梯度
- W_q, W_k 的梯度
- fc_out.weight 的梯度

每层包括:
- 梯度形状
- 梯度均值、标准差
- 梯度范围 [min, max]
```

### 5. **参数更新**
```
- 更新后的Embedding权重统计
```

## 🚀 如何使用

### 方法1: 使用详细训练脚本 (推荐)

```bash
python3 train_detailed.py
```

**特点:**
- 仅训练5个epoch，快速查看结果
- 只记录第1个epoch第1个batch的详细过程
- 日志保存到 `llm-log.txt`

### 方法2: 修改原有训练脚本

在 `train.py` 中已经集成了详细日志功能，会在第1个epoch的第1个batch自动记录。

```bash
python3 train.py
```

## 📖 日志解读示例

### 示例1: 注意力权重矩阵

```
Layer1_Attention - Head 0 的注意力权重矩阵 (每行和=1.0):
    1.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000  (和=1.0000)
    1.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000  (和=1.0000)
    1.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000  (和=1.0000)
    1.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000  (和=1.0000)
    0.9692 0.0000 0.0000 0.0000 0.0308 0.0000 0.0000 0.0000 0.0000  (和=1.0000)
```

**解读:**
- 第1行: Token 0 (START) 100%关注自己
- 第2行: Token 1 ("我") 100%关注第0个token (START)
- 第5行: Token 4 ("苹果") 96.92%关注第0个token，3.08%关注第4个token (自己)
- 右上角都是0: 因果mask防止看到未来的词

### 示例2: 注意力分数矩阵 (mask后)

```
Layer1_Attention - Head 0 的注意力分数矩阵 (mask后):
      7.01   -inf   -inf   -inf   -inf   -inf   -inf   -inf   -inf
     -8.78 -26.09   -inf   -inf   -inf   -inf   -inf   -inf   -inf
     28.79  -2.25  11.91   -inf   -inf   -inf   -inf   -inf   -inf
```

**解读:**
- 下三角矩阵结构清晰可见
- -inf表示被mask掉的位置（未来的词）
- 数值大小反映了Q和K的相似度

### 示例3: 梯度统计

```
embedding.weight:
  形状: [12, 64]
  梯度: 均值=0.000055, 标准差=0.006279, 范围=[-0.025222, 0.028205]
```

**解读:**
- 梯度均值接近0: 正常现象
- 标准差0.006: 梯度不会太大或太小
- 范围在合理区间内，没有梯度爆炸或消失

## 🔍 注意力机制计算流程

完整的计算步骤:

```
1. 输入 Token IDs [1, 7, 8, 6, 9, 2, ...]
              ↓
2. Embedding → [batch, seq_len, 64]
              ↓
3. 位置编码 → 添加位置信息
              ↓
4. Layer 1:
   - 输入 → Q, K, V 线性变换
   - 计算 Q @ K^T / sqrt(d_k)
   - 应用 Causal Mask
   - Softmax → 注意力权重
   - 注意力权重 @ V
   - 输出线性变换
   - 残差连接 + LayerNorm
   - 前馈网络
              ↓
5. Layer 2: (同上)
              ↓
6. 输出层 → Logits [batch, seq_len, vocab_size]
              ↓
7. 计算Loss
              ↓
8. 反向传播 → 记录梯度
              ↓
9. 参数更新
```

## 📈 学习过程观察要点

### 初始阶段 (Epoch 1, Batch 1)
- ✅ 注意力权重几乎全部集中在第一个token
- ✅ 损失值较大 (~2.6)
- ✅ 预测概率接近均匀分布 (~0.08-0.15)

### 训练后期
- ✅ 注意力开始分散到有意义的token
- ✅ 损失值降低 (~0.26)
- ✅ 预测概率更集中 (最高概率>0.9)

## 🎓 教学价值

这些详细日志可以帮助你:

1. **理解Transformer内部工作原理**
   - 看到Q、K、V矩阵的实际数值
   - 观察注意力是如何计算的
   - 理解Causal Mask的作用

2. **调试模型**
   - 检查梯度是否正常
   - 观察数值范围是否合理
   - 发现潜在的数值不稳定问题

3. **验证实现正确性**
   - 注意力权重每行和=1.0
   - Mask后的矩阵是下三角
   - 损失随训练降低

## 💡 自定义日志

如果想记录更多或更少的信息，可以修改:

1. `model.py` 中的 `log_details` 判断条件
2. `train_detailed.py` 中的 `log_details = (epoch == 0 and batch_idx == 0)`
3. 添加更多层的详细记录

例如，记录前3个epoch的所有batch:
```python
log_details = (epoch < 3)
```

## 📁 相关文件

- `train_detailed.py` - 详细日志训练脚本
- `train.py` - 标准训练脚本（含详细日志）
- `model.py` - 模型定义（含详细日志功能）
- `llm-log.txt` - 日志输出文件

## ⚠️ 注意事项

- 详细日志会显著增加日志文件大小
- 仅建议在第1个epoch的第1个batch记录详细过程
- 训练速度会略微降低（因为需要计算统计信息）
- 序列长度限制在10以内才显示完整矩阵

---

**祝学习愉快！** 🚀
